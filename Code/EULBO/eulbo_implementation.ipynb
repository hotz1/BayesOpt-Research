{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d90ef60b-b4ba-4b8c-a733-f9037bcdc5bd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# ELBO vs EULBO Comparison\n",
    "\n",
    "A Python notebook regarding Gaussian Processes based primarily on the pre-prints of two papers: *Computation-Aware Gaussian Processes* and *Approximation-Aware Bayesian Optimization*.\n",
    "\n",
    "## Problem Setup\n",
    "\n",
    "We want to use Gaussian Process regression to perform Bayesian optimization to find $x^{*} = \\arg\\max_{x \\in \\mathcal{X}}f(x)$, for the unknown objective function $f(\\cdot): \\mathcal{X} \\to \\mathbb{R}$, for an unknown real-valued function defined on the **compact** domain $\\mathcal{X} \\subset \\mathbb{R}^{d}$. Initially, we have some existing dataset $\\mathcal{D}_{0} = \\{(x_{i}, y_{i})\\}_{i=1}^{n}$, with $x_{i} \\in \\mathbb{R}^{d}, y_{i} \\in \\mathbb{R}$.\n",
    "\n",
    "Unfortunately, the standard `BayesOpt` formulation has $\\mathcal{O}(n^3)$ time complexity, as the \"proper\" mathematical formulation requires a matrix inversion. To reduce the computational complexity, we include an \"action matrix\" $\\mathbf{S}_{k} \\in \\mathbb{R}^{n \\times k}$ for $k \\ll n$ and performing Bayesian optimization on the \"simplified\" dataset $\\mathcal{D}'_{0} = (\\mathbf{S}_{k}^{\\top}\\mathbf{X}, \\mathbf{S}_{k}^{\\top}\\mathbf{y})$, which yields $\\mathcal{O}(kn^2)$ time complexity.\n",
    "\n",
    "In this notebook, we provide an altered Variational Inference (VI) approach to this problem. The variational family $\\mathcal{Q}_{n,k}$ of functions is indexed by the matrices (variational parameters) $\\mathbf{S}_k \\in \\mathbb{R}^{n \\times k}$.\n",
    "\n",
    "## Goal\n",
    "\n",
    "Beyond simply using the ELBO for the purposes of choosing a new action matrix and a new point for the dataset, we perform a joint optimization over the actions and new datapoints simultaneously, and this is done with a utility-weighted acquisition function (EULBO). \n",
    "\n",
    "The goal of this notebook is to compare the utility-weighted EULBO approach to ELBO-based approaches which do not incorporate a utility function and approaches for this maximization which perform individual acquisitions instead of optimizing over the new action(s) and new data point(s) in tandem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b841ece8-88a7-4066-9ab6-6f2b657ee492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    import jaxtyping\n",
    "except ImportError:\n",
    "    %pip install jaxtyping\n",
    "\n",
    "from typing import Optional, Tuple\n",
    "# Type hints are strictly optional, but personally I find that they make code more reasonable\n",
    "\n",
    "from jaxtyping import Float, Integer\n",
    "from collections.abc import Callable\n",
    "# This package allows type annotations that include the size of torch Tensors/numpy arrays\n",
    "# It's not necessary, but it helps with understanding what each function does\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Set DTYPE and DEVICE variables for torch tensors\n",
    "DTYPE = torch.float32\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "import torch\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "# Set a seed (for reproducibility)\n",
    "# torch.manual_seed(2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387c8387-479e-453d-b6b9-e020f88fdb2e",
   "metadata": {},
   "source": [
    "# Gaussian Process Setup\n",
    "\n",
    "For convenience, we will use a prior mean function $\\mu = 0$ and the [Matérn covariance function](https://en.wikipedia.org/wiki/Mat%C3%A9rn_covariance_function) with $\\nu = 5/2$ as the priors for our Bayesian optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e66d475b-d952-411e-a3bc-7dfc6f56ed5d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mu(X: Float[Tensor, \"N D\"]) -> Float[Tensor, \"N\"]:\n",
    "    r\"\"\"\n",
    "    Computes the (very lame) zero mean function mu(X) = 0\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.zeros(*X.shape[:-1], dtype=X.dtype, device=X.device)\n",
    "\n",
    "    # This return statement might seem like it's a pedantic way just to return the number 0 :)\n",
    "    # It's not:\n",
    "    # - if we want to compute a batch of GPs, the batch size of the returned zero\n",
    "    #   tensor will match the batch size of X\n",
    "    # - if X is a float64 tensor rather than float32, the returned zero tensor will match the correct dtype\n",
    "    # - if X is on the GPU rather than the CPU, the returned zero tensor will also be on the same device\n",
    "\n",
    "    # You don't always have to be this pedantic, but it's not a bad habit to get into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8efb0d51-576b-425d-8ab0-58d83945425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matern_kernel(\n",
    "    X1: Float[Tensor, \"M D\"], \n",
    "    X2: Float[Tensor, \"N D\"],\n",
    "    ls: Float[Tensor, \"1 D\"], \n",
    "    os: Float[Tensor, \"1 1\"],\n",
    ") -> Float[Tensor, \"M N\"]:\n",
    "    r\"\"\"\n",
    "    Computes Matern 5/2 kernel across all pairs of points (rows) in X1 & X2\n",
    "\n",
    "    k(X1, X2) = os * (1 + \\sqrt{5} * D + 5/3 * (D**2)) * exp(-\\sqrt{5} * D)\n",
    "    D = || (X1 - X2) / ls ||_2\n",
    "    https://en.wikipedia.org/wiki/Mat%C3%A9rn_covariance_function\n",
    "\n",
    "    ls: lengthscale\n",
    "    os: outputscale\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute D, D ** 2, \\sqrt{5} * D\n",
    "    D_sq = (X1.div(ls).unsqueeze(-2) - X2.div(ls).unsqueeze(-3)).square().sum(dim = -1)\n",
    "    # ^^^ This function is using broadcasting (via the unsqueeze operation)\n",
    "    #     to compute all of the pairwise distances in parallel\n",
    "    #\n",
    "    #     You should also get into the habit of using \"negative indexes\"\n",
    "    #     (i.e. unsqueeze(-2) rather than unsqueeze(0))\n",
    "    #     as negative indices allow you to easily write parallel code for batched operations.\n",
    "    #     (Again, not important now, but a good habit to develop!)\n",
    "    \n",
    "    D = torch.sqrt(D_sq + 1e-20)  # The 1e-20 is for numerical stability, so we don't get any NaNs if D≈0 but is very small and negative\n",
    "    \n",
    "    # Compute and return kernel\n",
    "    return torch.mul(\n",
    "        1 + (math.sqrt(5) * D) + ((5. / 3) * D_sq),\n",
    "        torch.exp(-math.sqrt(5) * D)\n",
    "    ).mul(os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "279b6bd9-ada5-42ea-82cf-254f3cc6d47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_posterior_mean_and_variance(\n",
    "    test_inputs: Float[Tensor, \"M D\"],\n",
    "    X: Float[Tensor, \"N D\"],\n",
    "    y: Float[Tensor, \"N\"],\n",
    "    K_chol: Float[Tensor, \"N N\"],\n",
    "    ls: Float[Tensor, \"1 D\"],\n",
    "    os: Float[Tensor, \"1 1\"],\n",
    ") -> tuple[Float[Tensor, \"M\"], Float[Tensor, \"M M\"]]:\n",
    "    r\"\"\"\n",
    "    Given inputs where we will evaluate the posterior, computes and returns the posterior moments\n",
    "    - E[ f(test_inputs) | y ] = mu(test_inputs) + k(test_inputs, X) @ k(X, X)^{-1} @ (y - mu(X))\n",
    "    - Cov[ f(test_inputs) | y ] = k(test_inputs, test_inputs) + k(test_inputs, X) @ k(X, X)^{-1} @ k(X, test_inputs)\n",
    "\n",
    "    test_inputs:     the matrix containing test inputs we want to evaluate f() on\n",
    "    X:               the matrix containing training inputs (where we have observations)\n",
    "    y:               is the vector of training observations\n",
    "    K_chol:          the Cholesky factor of the k(X, X) kernel matrix evaluated on training inputs\n",
    "                     plus observational noise\n",
    "                         i.e. K_chol @ K_chol.T = (k(X, X) + sigma^2 I)\n",
    "    ls:              is the lengthscale of the kernel\n",
    "    os:              is the outputscale of the kernel\n",
    "    \"\"\"\n",
    "\n",
    "    # ^^^ Note:\n",
    "    # You may be wondering why we are passing in K_chol,\n",
    "    # rather than computing it as part of this function.\n",
    "    # The reasoning will make sense once we start iteratively updating the posterior\n",
    "\n",
    "    # Compute k(X, X)^{-1} k(X, test_inputs)\n",
    "    # We need this term for both the posterior mean and posterior variance\n",
    "    Ktest = matern_kernel(X, test_inputs, ls, os)\n",
    "    K_inv_Ktest = torch.cholesky_solve(Ktest, K_chol, upper=False)\n",
    "\n",
    "    # ***SUPER IMPORTANT:***\n",
    "    # Note that we are using `cholesky_solve` to compute k(X, X)^{-1} k(X, test_inputs)\n",
    "    # rather than calling k(X, X).inverse() @ Ktest\n",
    "    # This is highly intentional\n",
    "    # 1) Never call .inverse(); it is highly unstable. Always perform matrix solves instead.\n",
    "    # 2) We can use the fact that k(X, X) is positive semi-definite to speed up solves.\n",
    "    #    The way we exploit this is by first applying the Cholesky decomposition to k(X, X)\n",
    "    #    (which exists because it is PSD) and then using it to perform a series of triangular\n",
    "    #    solves (which are fast).\n",
    "\n",
    "    # Compute posterior mean\n",
    "    posterior_mean = mu(test_inputs) + (K_inv_Ktest.mT @ (y - mu(X)).unsqueeze(-1)).squeeze(-1)\n",
    "    # Some notes:\n",
    "    # 1) .mT is the same as .T, but it works with batched tensors\n",
    "    #    i.e. it transposes the last two dimensions of the tensor\n",
    "    #    this helper is useful if we want to use batched GPs\n",
    "    # 2) (y - mu(X)).unsqueeze(-1) turns `y - mu(X)` from a N vector into a\n",
    "    #    N x 1 matrix so that we can then use the matmul function with it.\n",
    "    #    ().squeeze(-1) converts the resulting matmul back into a N vector.\n",
    "\n",
    "    # Compute posterior covariance\n",
    "    posterior_covar = matern_kernel(test_inputs, test_inputs, ls, os) - Ktest.mT @ K_inv_Ktest\n",
    "\n",
    "    # Done!\n",
    "    return posterior_mean, posterior_covar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1eedbbb-b367-43f3-b856-5f6d5d30a11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_chol_newdata(\n",
    "    K_chol: Float[Tensor, \"N N\"],\n",
    "    X: Float[Tensor, \"N D\"],\n",
    "    X_next: Float[Tensor, \"N_next D\"],\n",
    "    ls: Float[Tensor, \"1 D\"],\n",
    "    os: Float[Tensor, \"1 1\"],\n",
    "    sigma_sq: Float[Tensor, \"\"],\n",
    "    eps: float = 1e-4,\n",
    ") -> Float[Tensor, \"(N+N_next) (N+N_next)\"]:\n",
    "    \"\"\"\n",
    "    Computes the Cholesky factor of the block matrix\n",
    "    [ k(X, X) + sigma_sq * I          k(X, X_next)                      ]\n",
    "    [ k(X_next, X)                    k(X_next, X_next)  + sigma_sq * I ]\n",
    "    where k is the kernel covariance\n",
    "\n",
    "    This function should efficiently use prior computation.\n",
    "    Given that we already have computed K_chol @ K_chol.T = k(X, X),\n",
    "    we should be able to \"update\" that K_chol in O(N^2) time to get the\n",
    "    desired block cholesky factorization.\n",
    "\n",
    "    K_chol: Cholesky factorization of k(X, X) + sigma_sq * I\n",
    "    X: Prior data\n",
    "    X_next: Newly-added data\n",
    "    ls: Length scale of the kernel covariance\n",
    "    os: Output scale of the kernel covariance\n",
    "    sigma_sq: Observation noise\n",
    "    eps:      Small amount of noise to add to the diagonal for stability\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: this function is currently not efficient :)\n",
    "    # Right now, we're just computing the block Cholesky factorization from scratch,\n",
    "    #     rather than reusing K_chol for an efficient update\n",
    "    # You should make this function more efficient.\n",
    "\n",
    "    X_joint = torch.cat([X, X_next], dim = -2)\n",
    "    K_joint = matern_kernel(X_joint, X_joint, ls, os)\n",
    "\n",
    "    # Add sigma_sq * I to K_joint\n",
    "    #\n",
    "    # *Important*: the Cholesky factorization can be a bit numerically unstable\n",
    "    # If the smallest eigenvalues of K are ≈0, they can numerically appear to be slightly negative\n",
    "    #     which causes the Cholesky factorization to fail\n",
    "    # We can circumvent this problem by adding a small amount of diagonal noise to K_joint\n",
    "    #     to bump up all of the eigenvalues\n",
    "    I = torch.eye(K_joint.size(-1), dtype=K_joint.dtype, device=K_joint.device)\n",
    "    K_joint = K_joint + (sigma_sq + eps) * I\n",
    "\n",
    "    # Now we're ready to compute the Cholesky factor\n",
    "    # return torch.linalg.cholesky(K_joint, upper=False)\n",
    "\n",
    "    # Get the sub-blocks of K_joint\n",
    "    N = X.shape[0]\n",
    "    N_next = X_next.shape[0]\n",
    "    K_11, K_12, K_22 = K_joint[0:N, 0:N], K_joint[0:N, N:], K_joint[N:, N:]\n",
    "\n",
    "    # Cholesky factorization on the sub-blocks\n",
    "    L_21 = torch.linalg.solve_triangular(K_chol, K_12, upper = False).mT\n",
    "    L_22 = torch.linalg.cholesky(K_22 - L_21 @ L_21.mT, upper = False)\n",
    "\n",
    "    # Concatenate sub-blocks of Cholesky decomposition matrix and return them in the form\n",
    "    # [ K_chol      0 ]\n",
    "    # [ L_21     L_22 ]\n",
    "    return torch.cat(\n",
    "        (torch.cat((K_chol, torch.zeros(N, N_next)), dim = -1), \n",
    "         torch.cat((L_21, L_22), dim = -1)\n",
    "        ), dim = -2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced9ad90-4239-4343-b402-3c38626de059",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Test Functions\n",
    "\n",
    "We will use a few test functions to compare the performance of various Bayesian Optimization algorithms.\n",
    "\n",
    "These include:\n",
    "- The [\"Hartmann 6\" function](https://www.sfu.ca/~ssurjano/hart6.html) defined on $[0, 1]^{6}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bfd130b-ada6-4244-86ee-29794b4e5aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_periodic(X: Float[Tensor, \"N 1\"]) -> Float[Tensor, \"N\"]:\n",
    "    r\"\"\"\n",
    "    Computes values of f(x) = sin(2pi*x) + sin(4pi*x)\n",
    "    \"\"\"\n",
    "\n",
    "    ### TODO: Check if inputs are \"valid\" (possibly)\n",
    "\n",
    "    return X[:, 0].mul(2 * math.pi).sin() + X[:, 0].mul(4 * math.pi).sin()  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "242a8796-3a1f-4828-a69c-03eb7042862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hartmann_six(X: Float[Tensor, \"N 6\"]) -> Float[Tensor, \"N\"]:\n",
    "    r\"\"\"\n",
    "    Computes the value of the Hartmann six-dimensional test function on N rows of input data\n",
    "    More info on this test function at: https://www.sfu.ca/~ssurjano/hart6.html\n",
    "    \"\"\"\n",
    "\n",
    "    ### TODO: Check if inputs are \"valid\" (possibly)\n",
    "    \n",
    "    alpha = torch.tensor([1.0, 1.2, 3.0, 3.2], dtype = DTYPE, device = X.device)\n",
    "    A = torch.tensor([[10, 3, 17, 3.5, 1.7, 8],\n",
    "                      [0.05, 10, 17, 0.1, 8, 14],\n",
    "                      [3, 3.5, 1.7, 10, 17, 8],\n",
    "                      [17, 8, 0.05, 10, 0.1, 14]],\n",
    "                     dtype = DTYPE, device = X.device)\n",
    "    P = 1e-4 * torch.tensor([[1312, 1696, 5569, 124, 8283, 5886],\n",
    "                             [2329, 4135, 8307, 3736, 1004, 9991],\n",
    "                             [2348, 1451, 3522, 2883, 3047, 6650],\n",
    "                             [4047, 8828, 8732, 5743, 1091, 381]], \n",
    "                            dtype = DTYPE, device = X.device)\n",
    "\n",
    "    # Calculate \"inner sums\" \n",
    "    inner_sums: Float[Tensor, \"N 4\"] = torch.sum(A * (X.unsqueeze(-2) - P).pow(2), -1)\n",
    "\n",
    "    # Exponentiate and compute \"outer sums\"\n",
    "    outer_sums: Float[Tensor, \"N\"] = alpha @ torch.exp(-inner_sums).mT\n",
    "    \n",
    "    return outer_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b7014df-e9fe-446c-b2a3-894a1beecd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def observe(\n",
    "    func: Callable[[Float[Tensor, \"N D\"]], Float[Tensor, \"N\"]],\n",
    "    X: Float[Tensor, \"N D\"], \n",
    "    sigma_sq: Float = 1e-2,\n",
    ") -> Float[Tensor, \"N\"]:\n",
    "    r\"\"\"\n",
    "    A \"wrapper\" to return y = func(X) + noise.\n",
    "\n",
    "    func: A real-valued function defined on R^D which is applied row-wise to X\n",
    "    X: A matrix of N D-dimensional real-valued inputs to the function\n",
    "    sigma_sq: Variance of the IID observation noise \n",
    "    \"\"\"\n",
    "    \n",
    "    true_obs = func(X)\n",
    "    return true_obs + torch.randn_like(true_obs).mul(math.sqrt(sigma_sq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "036d9446-64ea-439e-bdfd-0db356910339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring the observe function works as intended\n",
    "# test_vecs = torch.rand(9, 6)\n",
    "# true_vals = hartmann_six(test_vecs)\n",
    "# obs_vals = observe(hartmann_six, test_vecs, sigma_sq = 1e-6) # Should be pretty similar.\n",
    "# print(true_vals)\n",
    "# print(obs_vals)\n",
    "# print(true_vals - obs_vals)\n",
    "\n",
    "# # Global optimum of the Hartmann 6D function\n",
    "# H6_argmax = torch.tensor([[0.20169, 0.150011, 0.476874, 0.275332, 0.311652, 0.6573]])\n",
    "# H6_globalmax = hartmann_six(H6_argmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd6af83-f0af-4194-9c16-383e5746dc60",
   "metadata": {},
   "source": [
    "# Acquisition\n",
    "\n",
    "The acquisition function which we utilize here is the EULBO; a utility-weighted version of the standard ELBO (Evidence Lower BOund) for variational inference which also includes a utility function.\n",
    "\n",
    "In this setting, we have the following:\n",
    "- $f(\\cdot): \\mathcal{X} \\to \\mathbb{R}$: A real-valued function defined on the **compact** set $\\mathcal{X} \\subset \\mathbb{R}^{d}$. \n",
    "- $\\mathcal{D}$: A dataset of observations $\\{(x_{i}, y_{i})\\}_{i=1}^{n}$, of the form $y_{i} = f(x_{i}) + \\epsilon_{i}$, where $x_{i} \\in \\mathcal{X} \\subset \\mathbb{R}^{d}, y_{i} \\in \\mathbb{R}$.\n",
    "- $\\mathcal{Q}$, a variational family of functions indexed by matrices $\\mathbb{S} \\in \\mathbb{R}^{n \\times k}$.\n",
    "- $u(x_{\\textrm{new}}, f; \\mathcal{D}): \\mathbb{R}^{d} \\to \\mathbb{R}$: A utility function for choosing $x_{\\textrm{new}} \\in \\mathbb{R}^{d}$.\n",
    "\n",
    "## EULBO\n",
    "\n",
    "Defining the **expected utility** function $\\alpha(\\mathbf{x}; \\mathcal{D}) = \\int u(x, f; \\mathcal{D})p(f \\mid \\mathcal{D})\\textrm{d}f$ yields the EULBO inequality:  $$\\log(\\alpha(\\mathbf{x}, \\mathcal{D})) \\ge \\mathbb{E}_{q_{\\mathbf{S}}}\\left[\\log\\left(\\frac{p(f, \\mathcal{D}_{t})}{q_{\\mathbf{S}}(f)}\\right)\\right] + \\mathbb{E}_{q_{\\mathbf{S}}}\\left[\\log\\left({u(x, f; \\mathcal{D}_{t})}\\right)\\right] - \\log(Z)$$\n",
    "\n",
    "This is the sum of the ELBO and the expected log-utility, with an additional constant term which we conveniently ignore, as we are maximizing the EULBO with respect to $(\\mathbf{S}, x_{\\textrm{new}})$ so this constant is not a concern as we do not care about the value of the bound outright.\n",
    "\n",
    "## Utility Functions\n",
    "\n",
    "There are several utility functions which may be useful for this goal. As we work with the expected log-utility, we must ensure that these functions are strictly positive. Possible utility functions include the following:\n",
    "- \"Soft\" Expected Improvement: $u_{\\textrm{SEI}}(x_{\\textrm{new}}, f, \\mathcal{D}) = \\log(1 + \\exp(f(x_{\\textrm{new}}) - y_{\\textrm{best}}))$\n",
    "- Z-Scored EI: $u_{\\textrm{ZEI}}(x_{\\textrm{new}}, f, \\mathcal{D}) = \\log\\left(1 + \\exp\\left(\\frac{f(x_{\\textrm{new}}) - y_{\\textrm{best}})}{\\sigma(x_{\\textrm{new}})}\\right)\\right)$, which is similar to $u_{\\textrm{SEI}}$ but with the difference weighted by the *approximate* standard deviation of $f(x_{\\textrm{new}})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "c64bdff1-c686-49e0-a642-604e3fdfe24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EULBO(\n",
    "    S: Float[Tensor, \"N T\"],\n",
    "    x_new: Float[Tensor, \"1 D\"],\n",
    "    X: Float[Tensor, \"N D\"],\n",
    "    y: Float[Tensor, \"N\"],\n",
    "    K_chol: Float[Tensor, \"N N\"],\n",
    "    n_sample: Integer = 1000\n",
    ") -> Float[Tensor, \" \"]:\n",
    "    \"\"\"\n",
    "    A function to compute the EULBO for an action matrix S and a datapoint x_new\n",
    "    based on observations from the existing dataset D = (X, y).\n",
    "\n",
    "    Parameters\n",
    "    S: A provied N-by-T action matrix (T << N)\n",
    "    x_new: A new input, where f(x_new) is defined\n",
    "    X: The \"input values\" in the observed dataset\n",
    "    y: The corresponding \"outputs\" for the observed dataset\n",
    "    K_chol: Cholesky decomposition of k(X, X) + sigma_sq * I\n",
    "\n",
    "    Global Constants     \n",
    "    ls: length scale of inputs\n",
    "    os: output scale\n",
    "    sigma_sq: Variance of observation noise\n",
    "\n",
    "    Returns\n",
    "    ELBO: The ELBO corresponding to conditioning on S'X and S'y\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cholesky decomposition of S'(K + sigma^2 I)S\n",
    "    STKS_chol = torch.linalg.cholesky(S.mT @ K_chol @ K_chol.mT @ S)\n",
    "\n",
    "    # Calculate the ELBO\n",
    "    ELBO_val = ELBO(S, X, y, K_chol, STKS_chol)\n",
    "    \n",
    "    # Monte Carlo samples to estimate expected log utility\n",
    "\n",
    "    # Compute the variational inference distribution q_S(f) = f|(S'D) at x_new\n",
    "    VI_mean, VI_var = compute_posterior_mean_and_variance(x_new, S.mT @ X, S.mT @ y, STKS_chol, ls, os)\n",
    "\n",
    "    # Get standard normal samples and reparametrize\n",
    "    z_samples: Float[Tensor, \"n_sample\"] = torch.normal(0, 1, size = (n_sample,))\n",
    "    VI_samples = VI_mean + VI_var.diag().sqrt() * z_samples # reparametrization trick\n",
    "\n",
    "    # Estimate softplus(expected improvement)\n",
    "    y_best = y.max()\n",
    "    uSEI = (VI_samples - y_best).exp().log1p()\n",
    "    mean_log_util = uSEI.log().mean()\n",
    "\n",
    "    return ELBO(S, X, y, K_chol, STKS_chol) + mean_log_utility(S, x_new, X, y, STKS_chol, n_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bef7e6c-2715-4b15-af18-547c335d4f95",
   "metadata": {},
   "source": [
    "From the Supplementary Material in the Computation Aware paper, we have that for an action matrix $\\mathbf{S}_{t} \\in \\mathbb{R}^{n \\times t}$, the ELBO is given by \n",
    "$$\\ell_{\\textrm{IterGP}}^{\\textrm{ELBO}} = \\frac{1}{2}\\left(\\frac{1}{\\sigma^{2}}\\lVert \\mathbf{y} - \\mu_{t}(\\mathbf{X})\\rVert_{2}^{2} +\\sum_{j=1}^{n}K_{t}(x_{j}, x_{j}) + n\\log(2\\pi\\sigma^2) - t\\log(\\sigma^2) + \\mathbf{y}^{\\top}\\mathbf{S}_{t}(\\mathbf{S}_{t}^{\\top}(\\mathbf{K}+\\sigma^{2}\\mathbf{I})\\mathbf{S}_{t})^{-1}\\mathbf{S}_{t}^{\\top}\\mathbf{K}\\mathbf{S}_{t}(\\mathbf{S}_{t}^{\\top}(\\mathbf{K}+\\sigma^{2}\\mathbf{I})\\mathbf{S}_{t})^{-1}\\mathbf{S}_{t}^{\\top}\\mathbf{y} - \\textrm{tr}\\left(\\left(\\mathbf{S}_{t}^{\\top}(\\mathbf{K}+\\sigma^{2}\\mathbf{I})\\mathbf{S}_{t}\\right)^{-1}\\mathbf{S}_{t}^{\\top}\\mathbf{K}\\mathbf{S}_{t}\\right) + \\log\\det\\left(\\mathbf{S}_{t}^{\\top}(\\mathbf{K}+\\sigma^{2}\\mathbf{I})\\mathbf{S}_{t}\\right) - \\log\\det\\left(\\mathbf{S}_{t}^{\\top}\\mathbf{S}_{t}\\right)\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "2be01c63-7502-47a5-8053-cf385fd1715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ELBO(\n",
    "    S: Float[Tensor, \"N T\"],\n",
    "    X: Float[Tensor, \"N D\"],\n",
    "    y: Float[Tensor, \"N\"],\n",
    "    K_chol: Float[Tensor, \"N N\"],\n",
    "    STKS_chol: Float[Tensor, \"T T\"]\n",
    ") -> Float[Tensor, \" \"]:\n",
    "    \"\"\"\n",
    "    A function to compute the ELBO for action matrix S based on the existing dataset D = (X, y).\n",
    "\n",
    "    Parameters\n",
    "    S: A queried action matrix \n",
    "    X: The \"input values\" in the observed dataset\n",
    "    y: The corresponding \"outputs\" for the observed dataset\n",
    "    K_chol: Cholesky decomposition of k(X, X) + sigma_sq * I\n",
    "    STKS_chol: Cholesky decomposition of S'(k(X, X) + sigma_sq * I)S\n",
    "\n",
    "    Global Constants     \n",
    "    ls: length scale of inputs\n",
    "    os: output scale\n",
    "    sigma_sq: Variance of observation noise\n",
    "\n",
    "    Returns\n",
    "    ELBO: The ELBO corresponding to conditioning on S'X and S'y\n",
    "    \"\"\"\n",
    "\n",
    "    # Get posterior of f | S'X, S'y\n",
    "    post_mean, post_var = compute_posterior_mean_and_variance(X, S_action.mT @ X, S_action.mT @ y, STKS_chol, ls, os)\n",
    "\n",
    "    # C = S (S'(K + sigma_sq * I)S)^(-1) S'\n",
    "    C = S @ torch.cholesky_solve(S.mT, STKS_chol)\n",
    "\n",
    "    # Add up the individual terms\n",
    "    ELBO = 0.5 * (\n",
    "        (y - post_mean).square().sum().div(sigma_sq) +\n",
    "        post_var.trace() +\n",
    "        S.shape[0] * math.log(2 * math.pi * sigma_sq) - S.shape[1] * math.log(sigma_sq) + \n",
    "        y @ C @ (K_chol @ K_chol.mT - sigma_sq * torch.eye(K_chol.shape[0])) @ C @ y -\n",
    "        torch.cholesky_solve(STKS_chol @ STKS_chol.mT - sigma_sq * S.mT @ S, STKS_chol).trace() +\n",
    "        torch.linalg.slogdet(STKS_chol @ STKS_chol.mT).logabsdet - torch.linalg.slogdet(S.mT @ S).logabsdet\n",
    "    )\n",
    "    return ELBO.view(1).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "6ecba1ff-c4ae-400f-a305-41531a9a9cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(534.1926)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean_log_utility(\n",
    "    S: Float[Tensor, \"N T\"],\n",
    "    x_new: Float[Tensor, \"1 D\"],\n",
    "    X: Float[Tensor, \"N D\"],\n",
    "    y: Float[Tensor, \"N\"],\n",
    "    STKS_chol: Float[Tensor, \"T T\"],\n",
    "    n_sample: Integer = 1000\n",
    ") -> Float[Tensor, \" \"]:\n",
    "    \"\"\"\n",
    "    A function to estimate the expected log utility for an action matrix S and a datapoint x_new\n",
    "    based on observations from the existing dataset D = (X, y).\n",
    "\n",
    "    Parameters\n",
    "    S: A provied N-by-T action matrix (T << N)\n",
    "    x_new: A new input, where f(x_new) is defined\n",
    "    X: The \"input values\" in the observed dataset\n",
    "    y: The corresponding \"outputs\" for the observed dataset\n",
    "    K_chol: Cholesky decomposition of k(X, X) + sigma_sq * I\n",
    "\n",
    "    Global Constants     \n",
    "    ls: length scale of inputs\n",
    "    os: output scale\n",
    "    sigma_sq: Variance of observation noise\n",
    "\n",
    "    Returns\n",
    "    ELBO: The ELBO corresponding to conditioning on S'X and S'y\n",
    "    \"\"\"\n",
    "    # Monte Carlo samples to estimate expected log utility\n",
    "\n",
    "    # Compute the variational inference distribution q_S(f) = f|(S'D) at x_new\n",
    "    VI_mean, VI_var = compute_posterior_mean_and_variance(x_new, S.mT @ X, S.mT @ y, STKS_chol, ls, os)\n",
    "\n",
    "    # Get standard normal samples and reparametrize\n",
    "    z_samples: Float[Tensor, \"n_sample\"] = torch.normal(0, 1, size = (n_sample,))\n",
    "    VI_samples = VI_mean + VI_var.diag().sqrt() * z_samples # reparametrization trick\n",
    "\n",
    "    # Estimate softplus(expected improvement)\n",
    "    y_best = y.max()\n",
    "    uSEI = (VI_samples - y_best).exp().log1p() # softplus\n",
    "    mean_log_util = uSEI.log().mean() \n",
    "\n",
    "    return mean_log_util.view(1).squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f29ef7-9b40-43ce-a05a-ce230da9621e",
   "metadata": {},
   "source": [
    "# Algorithm Comparison\n",
    "\n",
    "Lastly, we will compare two separate optimization schemes for this problem. These optimization schemes are:\n",
    "1. Joint Optimization: $(\\mathbf{S}_{t + 1}, {x}_{t+1}) = \\arg\\max_{(\\mathbf{S}, x)}\\left(\\texttt{EULBO}(\\mathbf{S}, {x}, \\mathbf{X}_{1:t}, \\mathbf{y}_{1:t})\\right)$\n",
    "2. Separate Optimization: $\\mathbf{S}_{t + 1} = \\arg\\max_{\\mathbf{S}}\\texttt{ELBO}(\\mathbf{S}, \\mathbf{X}_{1:t}, \\mathbf{y}_{1:t})$ and $x_{t+1} = \\arg\\max_{x \\in \\mathcal{X}}\\mathbb{E}_{Q_{\\mathbf{S}_{t}}}\\left[\\log(u(x; f, \\mathbf{X}_{1:t}, \\mathbf{y}_{1:t}))\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "aa7071d7-72c6-4e5b-8032-836cffa7102d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to ensure that each column of the matrix has an L2 norm of 1 \n",
    "\n",
    "def normalize_cols(S: Float[Tensor, \"R C\"]) -> Float[Tensor, \"R C\"]:\n",
    "    # Calculate L2 norms of columns of the given matrix\n",
    "    col_norms: Float[Tensor, \"C\"] = (S * S).sum(-2).sqrt()\n",
    "\n",
    "    # Error checking to avoid division by 0\n",
    "    if 0. in col_norms:\n",
    "        raise Exception(\"Error: One or more columns of the provided matrix has a norm of zero.\")\n",
    "\n",
    "    # Divide column-wise by the L2 norm\n",
    "    return torch.div(S, col_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2a0a283-949a-4783-a971-4228aca65ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "D: Integer = 6  # Dimension of function input space\n",
    "N: Integer = 100 # Initial dataset size\n",
    "\n",
    "# GP hyperparameters (global):\n",
    "#   - ls: kernel lengthscale\n",
    "#   - os: kernel outputscale\n",
    "#   - sigma_sq: observational noise\n",
    "ls: Float[Tensor, \"1 D\"] = torch.ones((1, D), dtype = DTYPE, device = DEVICE)\n",
    "os: Float[Tensor, \"1 1\"] = torch.tensor(1., dtype = DTYPE, device = DEVICE).view(1, 1)\n",
    "sigma_sq: Float[Tensor, \"1 1\"] = torch.tensor(1e-2, dtype = DTYPE, device = DEVICE).view(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93aaa902-1fba-4674-8e0d-88f700a435cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function which uses gradient descent to determine an N-by-t action matrix\n",
    "# def batch_action_matrix(\n",
    "#     K_chol: Float[Tensor, \"N N\"], \n",
    "#     t: Integer,\n",
    "#     num_iter: Integer = 500, \n",
    "#     lr: Float = 0.1\n",
    "# ) -> Float[Tensor, \"N t\"]:\n",
    "#     \"\"\"\n",
    "#     Uses gradient descent to determine the N-by-t action matrix which maximizes the difference in entropy.\n",
    "#     \"\"\"\n",
    "#     init_act = torch.eye(K_chol.shape[-1])[:, :t] # Somewhat arbitrary, but it is a normalized matrix with the right dimensions\n",
    "#     acts = torch.nn.Parameter(init_act)\n",
    "#     optimizer = torch.optim.Adam(params = [acts], lr = lr, maximize = True)\n",
    "#     iterator = tqdm.tqdm(range(num_iter), leave = False)\n",
    "    \n",
    "#     for _ in iterator:\n",
    "#         loss = entropy_info_gain(normalize_cols(acts), K_chol)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "#         iterator.set_postfix(loss = loss.item())\n",
    "\n",
    "#     return normalize_cols(acts.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e84f8c9-d846-4923-99fe-bf8f78c2506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function which uses gradient descent to determine an N-by-1 action matrix for the subsequent action\n",
    "# def iter_action_matrix(\n",
    "#     K_chol: Float[Tensor, \"N N\"],\n",
    "#     S_act: Float[Tensor, \"N m\"],\n",
    "#     num_iter: Integer = 500, \n",
    "#     lr: Float = 0.1\n",
    "# ) -> Float[Tensor, \"N 1\"]:\n",
    "#     \"\"\"\n",
    "#     Uses gradient descent to determine the next action (N-by-1 matrix) which maximizes the difference in entropy.\n",
    "#     \"\"\"\n",
    "#     act: Float[Tensor, \"N 1\"] = normalize_cols(torch.ones((K_chol.shape[-1], 1))) # Arbitrary, but it is a normalized matrix with the right dimensions\n",
    "#     next_act = torch.nn.Parameter(act)\n",
    "#     optimizer = torch.optim.Adam(params = [next_act], lr = lr, maximize = True)\n",
    "#     iterator = tqdm.tqdm(range(num_iter), leave = False)\n",
    "    \n",
    "#     for _ in iterator:\n",
    "#         all_acts = torch.cat((S_act, next_act), dim = -1)\n",
    "#         loss = entropy_info_gain(normalize_cols(all_acts), K_chol)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "#         iterator.set_postfix(loss = loss.item())\n",
    "\n",
    "#     return normalize_cols(next_act.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6252dec6-0630-438f-b653-f9815fcac8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def proj_data_NLL(\n",
    "#     S: Float[Tensor, \"N k\"], \n",
    "#     y: Float[Tensor, \"N\"], \n",
    "#     mu: Float[Tensor, \"N\"], \n",
    "#     K_chol: Float[Tensor, \"N N\"], \n",
    "#     sigma_sq: Float[Tensor, \"1 1\"]\n",
    "# ) -> Float[Tensor, \"1 1\"]:\n",
    "#     \"\"\"\n",
    "#     A function to compute the negative log-likelihood of projected data S^T y based on the equation above\n",
    "#     \"\"\"\n",
    "#     proj_y = S.mT @ y.unsqueeze(-1)\n",
    "#     proj_mu = S.mT @ mu.unsqueeze(-1)\n",
    "\n",
    "#     # Compute quadratic loss term in the NLL\n",
    "#     quad_loss = (proj_y - proj_mu).mT @ torch.linalg.solve(S.mT @ K_chol @ K_chol.mT @ S, proj_y - proj_mu)\n",
    "\n",
    "#     # Compute complexity term\n",
    "#     complexity = torch.linalg.slogdet(S.mT @ K_chol @ K_chol.mT @ S).logabsdet + S.shape[-1] * math.log(2 * math.pi)\n",
    "\n",
    "#     return 0.5 * (quad_loss + complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a00398dd-9d88-4166-b5e3-ff7f97a478b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compare_batch_iter(\n",
    "#     X: Float[Tensor, \"N D\"], \n",
    "#     y: Float[Tensor, \"N\"],\n",
    "#     K_chol: Float[Tensor, \"N N\"], \n",
    "#     sigma_sq: Float[Tensor, \"1 1\"], \n",
    "#     t_max: float\n",
    "# ) -> Tuple[float, float]:\n",
    "#     \"\"\"\n",
    "#     Compare negative log likelihoods of projected data from batch and iterative GP methods\n",
    "#     \"\"\"\n",
    "    \n",
    "#     ### TODO: Replace NLL with a better metric (e.g. test set MSE, posterior predictive NLL, f(x*) vs. true optimum)\n",
    "    \n",
    "#     S_batch = batch_action_matrix(K_chol, t_max)\n",
    "#     NLL_batch = proj_data_NLL(S_batch, y, mu(X), K_chol, sigma_sq)\n",
    "    \n",
    "#     S_iter = torch.zeros((X.shape[-2], 0))\n",
    "#     for i in range(t_max):\n",
    "#         S_next = iter_action_matrix(K_chol, S_iter)\n",
    "#         S_iter = torch.cat((S_iter, S_next), dim = -1)\n",
    "#     NLL_iter = proj_data_NLL(S_iter, y, mu(X), K_chol, sigma_sq)\n",
    "\n",
    "#     return NLL_batch.item(), NLL_iter.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd69e81-8e6f-4126-ac20-dc1783e03d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# D: Integer = 6  # Dimension of function input space\n",
    "# N: Integer = 100 # Initial dataset size\n",
    "\n",
    "# # GP hyperparameters:\n",
    "# #   - ls: kernel lengthscale\n",
    "# #   - os: kernel outputscale\n",
    "# #   - sigma_sq: observational noise\n",
    "# ls: Float[Tensor, \"1 D\"] = torch.ones((1, D), dtype = DTYPE, device = DEVICE)\n",
    "# os: Float[Tensor, \"1 1\"] = torch.tensor(1., dtype = DTYPE, device = DEVICE).view(1, 1)\n",
    "# sigma_sq: Float[Tensor, \"1 1\"] = torch.tensor(1e-2, dtype = DTYPE, device = DEVICE).view(1, 1)\n",
    "\n",
    "# t_values, batch_NLLs, iter_NLLs = [], [], []\n",
    "# for T in range(1, 6, 1):\n",
    "#     for b in range(25):\n",
    "#         # Randomly generate data \n",
    "#         X: Float[Tensor, \"N D\"] = torch.rand(N, D)\n",
    "#         y: Float[Tensor, \"N\"] = observe(hartmann_six, X, sigma_sq)\n",
    "#         K_chol = torch.linalg.cholesky(matern_kernel(X, X, ls, os) + (sigma_sq + 1e-4) * torch.eye(N))\n",
    "\n",
    "#         bNLL, iNLL = compare_batch_iter(X, y, K_chol, sigma_sq, t_max = T)\n",
    "#         t_values.append(T)\n",
    "#         batch_NLLs.append(bNLL)\n",
    "#         iter_NLLs.append(iNLL)\n",
    "\n",
    "#     # Progress update\n",
    "#     print(f\"Completed for {T} actions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab727596-98eb-4fcc-8ce2-09e1b4163785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# testing_df = pd.DataFrame({'Actions': t_values, \n",
    "#                            'Batch_NLL': batch_NLLs,\n",
    "#                            'Iter_NLL': iter_NLLs})\n",
    "# testing_df = testing_df.melt(id_vars = ['Actions'], var_name = \"Method\", value_name = \"NLL\")\n",
    "# sns.boxplot(testing_df, x = 'Actions', y = 'NLL', hue = 'Method', palette = 'Set2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599d16b3-59f3-434d-a01e-7e550947e51d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
