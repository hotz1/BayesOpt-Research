{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d90ef60b-b4ba-4b8c-a733-f9037bcdc5bd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Gradient Descent GP Selection\n",
    "\n",
    "A Python notebook regarding Gaussian Processes based primarily on the pre-prints of two papers: *Computation-Aware Gaussian Processes* and *Approximation-Aware Bayesian Optimization*.\n",
    "\n",
    "## Goal\n",
    "\n",
    "The main goal is to perform Bayesian Optimization via Gaussian Process (GP) regression, with tweaks to the existing algorithms to reduce computational space and time complexity via approximation.\n",
    "\n",
    "## Problem Setup\n",
    "\n",
    "Initially, we have some existing dataset $\\mathcal{D}_{0} = \\{(x_{i}, y_{i})\\}_{i=1}^{n}$, with $x_{i} \\in \\mathbb{R}^{d}, y_{i} \\in \\mathbb{R}$. Equivalently, we let $\\mathcal{D}_{0} = (\\mathbf{X}, \\mathbf{y})$, with $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}, \\mathbf{y} \\in \\mathbb{R}^{n}$. \n",
    "\n",
    "We want to use Gaussian Process regression to perform Bayesian optimization to find $x^{*} = \\arg\\max_{x \\in \\mathcal{X}}f(x)$, for the unknown objective function $f(\\cdot): \\mathcal{X} \\to \\mathbb{R}$, for some compact domain $\\mathcal{X} \\subset \\mathbb{R}^{d}$.\n",
    "\n",
    "Unfortunately, the standard `BayesOpt` formulation has $\\mathcal{O}(n^3)$ time complexity, as the \"proper\" mathematical formulation requires a matrix inversion. To reduce the computational complexity, we include an \"action matrix\" $\\mathbf{S} \\in \\mathbb{R}^{n \\times k}$ for $k \\ll n$ and performing Bayesian optimization on the \"simplified\" dataset $\\mathcal{D}'_{0} = (\\mathbf{S}^{\\top}\\mathbf{X}, \\mathbf{S}^{\\top}\\mathbf{y})$, which yields $\\mathcal{O}(kn^2)$ time complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b841ece8-88a7-4066-9ab6-6f2b657ee492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    import jaxtyping\n",
    "except ImportError:\n",
    "    %pip install jaxtyping\n",
    "\n",
    "from typing import Optional\n",
    "# Type hints are strictly optional, but personally I find that they make code more reasonable\n",
    "\n",
    "from jaxtyping import Float, Integer\n",
    "# This package allows type annotations that include the size of torch Tensors/numpy arrays\n",
    "# It's not necessary, but it helps with understanding what each function does\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Set DTYPE and DEVICE variables for torch tensors\n",
    "DTYPE = torch.float32\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Set a seed (for reproducibility)\n",
    "# torch.manual_seed(2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e66d475b-d952-411e-a3bc-7dfc6f56ed5d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mu(X: Float[Tensor, \"N D\"]) -> Float[Tensor, \"N\"]:\n",
    "    r\"\"\"\n",
    "    Computes the (very lame) zero mean function mu(X) = 0\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.zeros(*X.shape[:-1], dtype=X.dtype, device=X.device)\n",
    "\n",
    "    # This return statement might seem like it's a pedantic way just to return the number 0 :)\n",
    "    # It's not:\n",
    "    # - if we want to compute a batch of GPs, the batch size of the returned zero\n",
    "    #   tensor will match the batch size of X\n",
    "    # - if X is a float64 tensor rather than float32, the returned zero tensor will match the correct dtype\n",
    "    # - if X is on the GPU rather than the CPU, the returned zero tensor will also be on the same device\n",
    "\n",
    "    # You don't always have to be this pedantic, but it's not a bad habit to get into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8efb0d51-576b-425d-8ab0-58d83945425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matern_kernel(X1: Float[Tensor, \"M D\"], X2: Float[Tensor, \"N D\"],\n",
    "                  ls: Float[Tensor, \"1 D\"], os: Float[Tensor, \"1 1\"], ) -> Float[Tensor, \"M N\"]:\n",
    "    r\"\"\"\n",
    "    Computes Matern 5/2 kernel across all pairs of points (rows) in X1 & X2\n",
    "\n",
    "    k(X1, X2) = os * (1 + \\sqrt{5} * D + 5/3 * (D**2)) * exp(-\\sqrt{5} * D)\n",
    "    D = || (X1 - X2) / ls ||_2\n",
    "    https://en.wikipedia.org/wiki/Mat%C3%A9rn_covariance_function\n",
    "\n",
    "    ls: lengthscale\n",
    "    os: outputscale\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute D, D ** 2, \\sqrt{5} * D\n",
    "    D_sq = (X1.div(ls).unsqueeze(-2) - X2.div(ls).unsqueeze(-3)).square().sum(dim = -1)\n",
    "    # ^^^ This function is using broadcasting (via the unsqueeze operation)\n",
    "    #     to compute all of the pairwise distances in parallel\n",
    "    #\n",
    "    #     You should also get into the habit of using \"negative indexes\"\n",
    "    #     (i.e. unsqueeze(-2) rather than unsqueeze(0))\n",
    "    #     as negative indices allow you to easily write parallel code for batched operations.\n",
    "    #     (Again, not important now, but a good habit to develop!)\n",
    "    \n",
    "    D = torch.sqrt(D_sq + 1e-20)  # The 1e-20 is for numerical stability, so we don't get any NaNs if Dâ‰ˆ0 but is very small and negative\n",
    "    \n",
    "    # Compute and return kernel\n",
    "    return torch.mul(\n",
    "        1 + (math.sqrt(5) * D) + ((5. / 3) * D_sq),\n",
    "        torch.exp(-math.sqrt(5) * D)\n",
    "    ).mul(os)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced9ad90-4239-4343-b402-3c38626de059",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Test Functions\n",
    "\n",
    "We will use a few test functions to compare the performance of various Bayesian Optimization algorithms.\n",
    "\n",
    "These include:\n",
    "- A simple periodic function $f(x) = \\sin(2\\pi{x}) + \\sin(4\\pi{x})$ defined on $[-1, 1] \\subset \\mathbb{R}$.\n",
    "- The [\"Hartmann 6\" function](https://www.sfu.ca/~ssurjano/hart6.html) defined on $[0, 1]^{6}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfd130b-ada6-4244-86ee-29794b4e5aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_periodic(X: Float[Tensor, \"N 1\"]) -> Float[Tensor, \"N\"]:\n",
    "    r\"\"\"\n",
    "    Computes values of f(x) = sin(2pi*x) + sin(4pi*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "242a8796-3a1f-4828-a69c-03eb7042862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hartmann_six(X: Float[Tensor, \"N 6\"]) -> Float[Tensor, \"N\"]:\n",
    "    r\"\"\"\n",
    "    Computes the value of the Hartmann six-dimensional test function on N rows of input data\n",
    "    More info on this test function at: https://www.sfu.ca/~ssurjano/hart6.html\n",
    "    \"\"\"\n",
    "\n",
    "    ### TODO: Check if inputs are \"valid\" (possibly)\n",
    "    \n",
    "    alpha = torch.tensor([1.0, 1.2, 3.0, 3.2], dtype = DTYPE, device = X.device)\n",
    "    A = torch.tensor([[10, 3, 17, 3.5, 1.7, 8],\n",
    "                      [0.05, 10, 17, 0.1, 8, 14],\n",
    "                      [3, 3.5, 1.7, 10, 17, 8],\n",
    "                      [17, 8, 0.05, 10, 0.1, 14]],\n",
    "                     dtype = DTYPE, device = X.device)\n",
    "    P = 1e-4 * torch.tensor([[1312, 1696, 5569, 124, 8283, 5886],\n",
    "                             [2329, 4135, 8307, 3736, 1004, 9991],\n",
    "                             [2348, 1451, 3522, 2883, 3047, 6650],\n",
    "                             [4047, 8828, 8732, 5743, 1091, 381]], \n",
    "                            dtype = DTYPE, device = X.device)\n",
    "\n",
    "    # Calculate \"inner sums\" \n",
    "    inner_sums: Float[Tensor, \"N 4\"] = torch.sum(A * (X.unsqueeze(-2) - P).pow(2), -1)\n",
    "\n",
    "    # Exponentiate and compute \"outer sums\"\n",
    "    outer_sums: Float[Tensor, \"N\"] = -alpha @ torch.exp(-inner_sums).mT\n",
    "    \n",
    "    return outer_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "036d9446-64ea-439e-bdfd-0db356910339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0051, -3.3224])\n"
     ]
    }
   ],
   "source": [
    "# Just checking the function works as desired\n",
    "# test_vecs = torch.tensor([[0., 0., 0., 0., 0., 0.],\n",
    "#                           [0.20169, 0.150011, 0.476874, 0.275332, 0.311652, 0.657300]])\n",
    "# print(hartmann_six(test_vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc899a50-b8e3-474c-bb86-9d8549035d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.5518e-09, 1.2797e-10, 5.5518e-09, 1.2797e-10],\n",
       "        [1.2489e-08, 3.0514e-10, 1.1287e-08, 2.5036e-10]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
