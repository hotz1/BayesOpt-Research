{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e82b4708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm.notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "729eb011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-0.9771,  1.4426], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.7405, -1.5294, -0.4147, -0.7225,  2.3669], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Parameters that we want to optimize over\n",
    "\n",
    "X1 = torch.nn.Parameter(torch.randn(2))  # torch.nn.Parameter marks a Tensor as one that we want to optimize\n",
    "X2 = torch.nn.Parameter(torch.randn(5))\n",
    "\n",
    "print(X1)\n",
    "print(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cb2a914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ^^ note the \"requires_grad=True\" with the tensor - this means that\n",
    "# it will accumulate gradients from automatic differentiation, which we can\n",
    "# use with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2782da4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = torch.randn(X1.size(-1), X2.size(-1))\n",
    "\n",
    "# Here's a simple toy objective function\n",
    "# We want to find the optimal rank-1 approximation of mat\n",
    "# i.e.\n",
    "# argmin_{X1, X2} || mat - X1 @ X2^T ||^2_F\n",
    "#\n",
    "def objective_function(x, S):\n",
    "    return (mat - X1.unsqueeze(-1) @ X2.unsqueeze(-2)).norm().square()\n",
    "\n",
    "# ^^^ This objective function is non-convex, so there's no guarantee that \n",
    "# a gradient-based optimization should find the global minimum\n",
    "# However, gradient-based optimization is the standard approach these days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97473e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18.1362, grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# With the random initial values of X1 and X2, our objective function shouldn't be that good\n",
    "print(objective_function(X1, X2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32f67e16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7bd55ad75e24c8eb69f7d70a70826eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now here's an optimization loop\n",
    "\n",
    "optimizer = torch.optim.Adam(params=[X1, X2], lr=0.1)\n",
    "# ^^ Adam is a gradient descent optimizer that does some fancy geometry stuff for faster optimization\n",
    "# lr = learning rate. 0.1 is a good starting point, though you maybe want to play with this hyperparameter\n",
    "\n",
    "#############\n",
    "# The gradient descent loop\n",
    "#############\n",
    "num_iter = 400  # You may need more iterations than this\n",
    "iterator = tqdm.tqdm(range(num_iter))\n",
    "\n",
    "for _ in iterator:\n",
    "    loss = objective_function(X1, X2)  # Using the current values of x and S, compute the obj. function\n",
    "        # Should be a scalar\n",
    "    loss.backward()  # This line runs backpropagation to compute d loss / dx and d loss / dS\n",
    "        # If you peak at x.grad and S.grad at this line, you will see a tensor that corresponds\n",
    "        # to these derivatives\n",
    "    optimizer.step()  # This line performs the gradient update, using whatever is stored in\n",
    "        # x.grad and S.grad\n",
    "        # x and S will now be updated\n",
    "    optimizer.zero_grad()  # This line resets whatever is stored in d loss / dx and d loss / dS\n",
    "    \n",
    "    # Let's see the loss going down over time\n",
    "    iterator.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79199402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-0.4659,  0.9794], requires_grad=True) Parameter containing:\n",
      "tensor([ 0.2173, -0.9189,  1.4809,  0.5069,  1.9396], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(X1, X2)  # They're different values now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10ca8711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9370, grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# And how good is our objective function now?\n",
    "print(objective_function(X1, X2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70c10b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ^^^ better!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
